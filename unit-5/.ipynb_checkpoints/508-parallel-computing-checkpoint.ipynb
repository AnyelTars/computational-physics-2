{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5881b2f0",
   "metadata": {},
   "source": [
    "# Code paralellisation\n",
    "\n",
    "## How can we make our code run faster?\n",
    "\n",
    "1. Optimize the code.\n",
    "\n",
    "\n",
    "2. Move computationally demanding parts of the code from an interpreted language (Python, Ruby, etc.) to a compiled language (C/C++, Julia, Rust, etc.)\n",
    "\n",
    "\n",
    "3. Use better theoretical methods that require less computation for the same accuracy.\n",
    "\n",
    "\n",
    "4. A different strategy for speeding up codes is **parallelization**, in which we split the computational work among multiple processing units that labor simultaneously.\n",
    "\n",
    "## Processing units:\n",
    "\n",
    "The **processing units** might include central processing units (CPUs), graphics processing units (GPUs), vector processing units (VPUs), or something similar.\n",
    "\n",
    "- Multiple processing units can complete a calculation faster than a single processing unit.\n",
    "\n",
    "\n",
    "- For example, if a calculation takes 1 hour to run using one CPU, it might be possible to parallelize the work of the calculation across two CPUs and run it in only 30 minutes.\n",
    "\n",
    "\n",
    "- Note that parallelization cannot reduce the total amount of computational work required to run a calculation; in fact, it generally introduces additional work associated with communication and coordination between the processing units.\n",
    "\n",
    "\n",
    "- In general, if a calculation takes **t** hours to run in serial (that is, on a single processing unit), it will take at least **t/n** hours to run on n processing units. tTis principle is more formally expressed through **Amdahl’s law** (https://en.wikipedia.org/wiki/Amdahl%27s_law). A primary goal of parallelization is to ensure that the actual parallelized runtimes are as close to the ideal runtime of **t/n** as possible.\n",
    "\n",
    "## What is High-Performance Computing?\n",
    "\n",
    "The field of high performance computing (HPC) takes this concept of parallelization to its logical limit:\n",
    "\n",
    "- Rather than parallelizing over just a handful of processing units, such as those you might find on a typical desktop or laptop computer, HPC applications involve the use of supercomputers that might consist of many thousands of processing units.\n",
    "\n",
    "\n",
    "- Parallelising codes to such scales is often very difficult and requires very good management. \n",
    "\n",
    "#### Reference:\n",
    "https://education.molssi.org/parallel-programming/01-introduction/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3112ff3",
   "metadata": {},
   "source": [
    "### Types of Parallelization\n",
    "\n",
    "#### 1. Distributed-memory parallelization.\n",
    "\n",
    "In this approach, multiple instances of the same executable, called “processes,” are run simultaneously, with each process being run on a different core. Each process has its own independent copy of any information required to run the simulation; in other words, if you run a calculation with 2 processes, it might require twice as much memory as a calculation run with a single process (although there are some things you can do to mitigate this).\n",
    "\n",
    "\n",
    "#### 2. Shared-memory parallelization.\n",
    "\n",
    "In this approach, multiple “threads” are run using a single, shared memory allocation in RAM. This has the advantage of being more memory efficient than distributed-memory parallelism, but because all threads must have access to the same memory, it cannot be used for inter-node parallelization.\n",
    "\n",
    "\n",
    "#### 3. Vectorization.\n",
    "\n",
    "Vectorization takes advantage of that modern CPU cores support “single instruction, multiple data” (SIMD), which means that they can perform the same operation on multiple operands simultaneously. For example, if you want to multiply each element of a vector by a scalar, SIMD can enable individual cores to perform this multiplication on multiple elements of the vector simultaneously. Vectorization is largely handled by the compiler (although there are various ways you can influence when and how the compiler vectorizes code) and is not covered in these lessons.\n",
    "\n",
    "#### 4. Heterogeneous computing (e.g. GPUs, FPGAs, etc.).\n",
    "\n",
    "One of the big trends in HPC in recent years is the use of GPU acceleration, in which parts of a calculation are run on a CPU while other parts of the calculation are run on a GPU.\n",
    "\n",
    "\n",
    "The above forms of parallelization are not mutually exclusive. Codes can benefit from vectorization and GPU-acceleration in addition to other parallelization techniques. Notably, one popular approach to parallelization is to use shared-memory parallelization to handle intra-node parallelization, while using a distributed-memory parallelization technique to handle inter-node parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec59bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
