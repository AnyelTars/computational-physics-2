{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299d42dc",
   "metadata": {},
   "source": [
    "# High-performance computing:\n",
    "\n",
    "## HPC Architecture\n",
    "\n",
    "\n",
    "Understanding how to parallelize software requires first having a high-level understanding of the design of the physical machines that run the software.\n",
    "\n",
    "The image below shows a rough overview of some of the elements of a typical supercomputer.\n",
    "\n",
    "![hpc_architecture.png](attachment:hpc_architecture.png)\n",
    "\n",
    "- The supercomputer consists of a large number of nodes. Only four nodes are shown in the picture, but modern supercomputers can have thousands of nodes that are interconnected in a large “cluster.”\n",
    "\n",
    "\n",
    "- In some respects, we can think of each node as an independent computer, connected to many other computers within a local network.\n",
    "\n",
    "\n",
    "- Each node has a group of “cores,” which are the microprocessors responsible for doing computational work.\n",
    "\n",
    "\n",
    "- In the earlier days of computing, all CPUs had a single core; today, nearly all CPUs are multi-core, including those used in desktops and laptops.\n",
    "\n",
    "## Cedia cluster:\n",
    "\n",
    "CEDIA provides HPC services in Ecuador, see the documentation below:\n",
    "\n",
    "https://www.cedia.edu.ec/infraestructura/supercomputador\n",
    "\n",
    "\n",
    "To use it:\n",
    "\n",
    "1. First you need to apply for an account (see lecture 507).\n",
    "\n",
    "\n",
    "2. Then, you can log into the cluster using SSH:\n",
    "\n",
    "```\n",
    "    ssh -XY -i ~/.ssh/your_cedia_publickey wladimir.banda@hpc.cedia.edu.ec\n",
    "```\n",
    "\n",
    "The option **-XY** is for accessing graphics. \n",
    "\n",
    "\n",
    "3. Now, we will use SLURM commands to see what is available:\n",
    "\n",
    "#### a) sinfo:\n",
    "\n",
    "```\n",
    "[wladimir.banda@login1 ~]$ sinfo\n",
    "PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\n",
    "batch        up   infinite      1    mix dgx-node-0-1\n",
    "mig*         up   infinite      1    mix dgx-node-0-0\n",
    "cpu-dev      up 5-00:00:00      2    mix dgx-node-0-[0-1]\n",
    "cpu          up 2-00:00:00      2    mix dgx-node-0-[0-1]\n",
    "cpu-max      up 1-00:00:00      2    mix dgx-node-0-[0-1]\n",
    "gpu-dev      up 4-00:00:00      2    mix dgx-node-0-[0-1]\n",
    "gpu          up 2-00:00:00      2    mix dgx-node-0-[0-1]\n",
    "gpu-max      up 1-00:00:00      2    mix dgx-node-0-[0-1]\n",
    "```\n",
    "\n",
    "#### b) scontrol show partition:\n",
    "\n",
    "```\n",
    "[wladimir.banda@login1 ~]$ scontrol show partition\n",
    "PartitionName=batch\n",
    "   AllowGroups=ALL AllowAccounts=batch_group AllowQos=ALL\n",
    "   AllocNodes=ALL Default=NO QoS=N/A\n",
    "   DefaultTime=NONE DisableRootJobs=NO ExclusiveUser=NO GraceTime=0 Hidden=NO\n",
    "   MaxNodes=UNLIMITED MaxTime=UNLIMITED MinNodes=0 LLN=NO MaxCPUsPerNode=UNLIMITED\n",
    "   Nodes=dgx-node-0-1\n",
    "   PriorityJobFactor=1 PriorityTier=1 RootOnly=NO ReqResv=NO OverSubscribe=NO\n",
    "   OverTimeLimit=NONE PreemptMode=OFF\n",
    "   State=UP TotalCPUs=256 TotalNodes=1 SelectTypeParameters=NONE\n",
    "   JobDefaults=(null)\n",
    "   DefMemPerCPU=3824 MaxMemPerNode=UNLIMITED\n",
    "   TRES=cpu=256,mem=980288M,node=1,billing=256,gres/gpu=8,gres/gpu:a100-sxm4-40gb=8\n",
    "```\n",
    "\n",
    "#### c) squeue:\n",
    "\n",
    "```\n",
    "[wladimir.banda@login1 ~]$ squeue\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "              8572     batch no-shell patricia  R 49-21:44:28      1 dgx-node-0-1\n",
    "             10569       cpu interact henry.ma  R    1:16:06      1 dgx-node-0-1\n",
    "             10568   cpu-max sys/dash paul.bra  R    1:22:46      1 dgx-node-0-0\n",
    "             10549       gpu no-shell krishna.  R 1-10:22:24      1 dgx-node-0-0\n",
    "             10566       gpu interact bonny.ba  R    3:20:27      1 dgx-node-0-0\n",
    "             10440   gpu-dev no-shell carlos.m  R 3-00:09:24      1 dgx-node-0-0\n",
    "             10546   gpu-dev sys/dash osvaldo.  R 1-14:52:03      1 dgx-node-0-0\n",
    "             10561   gpu-dev sys/dash osvaldo.  R   18:22:55      1 dgx-node-0-0\n",
    "```\n",
    "\n",
    "#### d) squeue -u $USER\n",
    "\n",
    "```\n",
    "[wladimir.banda@login1 ~]$ squeue -u $USER\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "```\n",
    "\n",
    "#### e) salloc (interactive jobs)\n",
    "\n",
    "```\n",
    "salloc -p cpu -n 1 --mem=1G\n",
    "```\n",
    "\n",
    "\n",
    "This reserves 1 node and 1GB of memory:\n",
    "\n",
    "```\n",
    "[wladimir.banda@login1 ~]$ salloc -p cpu -n 1 --mem=1G\n",
    "salloc: Granted job allocation 10570\n",
    "salloc: Waiting for resource configuration\n",
    "salloc: Nodes dgx-node-0-0 are ready for job\n",
    "[wladimir.banda@login1 ~]$ \n",
    "```\n",
    "\n",
    "#### f) SSH into compute node:\n",
    "\n",
    "```\n",
    "ssh dgx-node-0-0\n",
    "```\n",
    "\n",
    "\n",
    "#### g) Check again:\n",
    "\n",
    "```\n",
    "wladimir.banda@dgx-node-0-0:~$ squeue -u $USER\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "             10570       cpu interact wladimir  R       3:32      1 dgx-node-0-0\n",
    "```\n",
    "\n",
    "#### h) Use scp to transfer files:\n",
    "\n",
    "```\n",
    "scp -i ~/.ssh/id_rsa_oldcedia example_mpi10.py wladimir.banda@hpc.cedia.edu.ec:/home/wladimir.banda/.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e012c8",
   "metadata": {},
   "source": [
    "### BUILDING MPI:\n",
    "\n",
    "Get the distro from here:\n",
    "\n",
    "https://www.open-mpi.org/software/ompi/v4.1/\n",
    "\n",
    "```\n",
    "wget https://www.open-mpi.org/software/ompi/v4.1/\n",
    "```\n",
    "Follow this:\n",
    "\n",
    "https://mpi4py.readthedocs.io/en/stable/appendix.html#building-mpi\n",
    "\n",
    "\n",
    "```\n",
    "$ tar -zxf openmpi-X.X.X tar.gz\n",
    "$ cd openmpi-X.X.X\n",
    "$ ./configure --prefix=/usr/local/openmpi\n",
    "$ make all\n",
    "$ make install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471470a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
